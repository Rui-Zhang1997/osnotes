Definitional
Functional
Mastery

WORDS?!
--------------------------
Signal mask
Banker's algorithm
Tree/Spin Barriers

--------------------------
Caching

Invoking the OS
    Interrupts, traps, and exceptions (voluntary/involuntary, user/non-user, how they work, line interrupts)

I/O event notification
    Polling
    Fetch-Decode-Execute
        Computer as a large, general-purpose calculator
        All von Neumann computers follow same loop  
            - Fetch next instruction from memory
            - Decode
            - Execute
        PC counter points to next instruction to run
        Indirection: fetch contents of a shell "pointed to" by the cell in question
        Steal an operand bit to signify if an indrection is desired
        Branch Instructions
            If content of this cell is [zero, non-zero, etc] set PC to this location
            jump is unconditional branch
        Operands and dst can be in
            Registers only (Sparc, PowerPC, MIPS, Alpha)
            Registers & 1 memory operand (x86 and clones)
            Any combination of registers and memory (Vax)
            Operations 100-1000 times faster when operands are in registers compared to when they are in memory
            Save instruction space too
                Only 176-32 registers, not GB of memory

    Operating System virtualized the hardware

Send/receive data to I/O devices

Processes and Threads
    - Activation record is the currently executing stack frame

    THREAD VS PROCESS CONTEDXT
        PROCESSES
            - Separate execution context
        THREAD
            - Runs on the same execution context on a different stack

    - Each process can have multiple threads
    - Each thread has a private stack
        - Registers are also private
    - All threads share code, globals, and heap
        - Objects shared across multiple threads should be in the heap or globals
    DIAGRAM 1

    SINGLE-THREADED ADDR SPACE

    +-----------------+
    |       OS        |
    +-----------------+
    |      Code       |
    +-----------------+
    |     Globals     |
    +-----------------+
    |                 |
    |      Stack      |
    |                 |
    +-----------------+
    |                 |
    +-----------------+
    |      Heap       |
    +-----------------+

    MULTI-THREADED ADDR SPACE

    +-----------------+
    |       OS        |
    +-----------------+
    |       Code      |
    +-----------------+
    |     Globals     |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Heap       |
    +-----------------+

    - Each thread described by a thread-control block (TCB)
        - Contains
            - Thread ID
            - Space for saving registers
            - Thread-specific info (e.g. signal mask, scheduling priority)
    - Although model is each thread ahs private stack, threads actually share process addr space (NO MEMORY PROTECTION)
    - Threads could potentially write into each other's stacks

    RUNNING ON A SINGLE CORE NON-MULTITHREADED CPU wtf to do?
        - We multiplex multiple threads on the core
        - Only one thread runs at one time
        - OS may decide to stop currently running thread and allow another to run (called context-switching)
   
    CONTEXT SWITCHING
        - Copy all live registers to TCB
            - For register-only machines, need at least 1 scratch register
                - points to area of memory in TCB that registers should be saved to
        - Restore state of thread to run next
            - Copy values of live registers from TCB to registers
        - When does context switching take place?
            - When OS decides a thread has run long enough 
            - When thread is performing I/O
            - To wait for some other thread
        - At entry to context switch, return addr is either in a register or on the stack (in current activation record)
        - CS saves this return addr to the TCB instead of current PC
        - To thread, it looks like CS just took a while to return
        - Run queue that points to TCB of threads ready to run
        - Blocked queue per device to hold TCBs of threads blocked waiting for I/O on device to complete
        - When thread is switched out at timer interrupt, it is still ready to run so TCB stays on the run queue
        - When thread is switched out because it is blocking in I/O, TCB is moved to the blocked queue of the device

        SWITCHING CODE PATHWAY
            user thread executing -> clock interrupt -> PC modified by hardware to "vector" to interrrupt handler ->
            user thread state is saved for restart -> clock interrupt handler is invoked -> disable interrupt checking ->
            check whether current thread has run "long enough" -> if yes,m post asynchronous software trap (AST) ->
            enable interrupt checking -> exit clock interrupt handler -> enter "return to user" code -> check whether AST was posted ->
            if not, restore user thread state and return 4to executing user thread; if AST was posted, call context switch code

        ALTERNATIVE
            user thread executing -> system call to perform I/O -> PC modified by hardware to "vector" to trap handler ->
            user thread state saved for restart -> OS code to perform system call is invoked -> disable interrupt checking ->
            I/O operation started (by invoking I/O driver) -> set thread status to waiting -> move thread's TCB from run queue to wait
            queue associates with specific device -> enable interrupt checking -> exit trap handler -> call context switching code

        - At entry to CS, return addr is either in register or on the stack (in the current activation record). CS saves return addr to TCB
        instead of current PC
        - A run queue points to TCBs of threads ready to run
        - Blocked queue per device to holdw TCBs of threads blocked waiting for an I/O operation on that device
        - When thread is switched out at timer interrupt it is still ready to run so its TCB stays on the run queue
        - When a thread is switched out b/c it is blocking on I/O, TCB is moved to blocked queue of that device
        
        SWITCHING TO THREAD OF DIFF. PROCESS
        - Caches:
            Physical addresses: no issue
            Virtual addrs: cache must either have process tag or must flush cache on CS
        - TLB
            Each entry must have process tag or must flush TLB on CS
        - Page Table
            Typically have page table pointer (register) that must be reloaded on CS
        
        What happens if kernel wants to signal a process when all of its threads are locked?
        When there are multiple threads, which thread should the kernel deliver the signal to?
            - OS writes into PCB that signal should be deliviered
            - Next time thread from process is allowed to run, signal is delivered to that thread as part of the CS
        What happens if kernel needs to deliver multiple signals?
        
        Kernel-level threads
            Sees multiple execution contexts
            Thread management done by kernel
        User-level threads
            Implemented a sa thread library
            Kernel sees one execution context and is unaware of thread activity
            Can be preemptive or not

        Advantages of ULT
            Performance: low-cost thread operations
            Flexibility: scheduling can be application specific
            Portability: user-level thread library easy to port
        Disadvantages:
            If a user-level thread blocks in kernel entire process is blocked
            Cannot take advantage of multiprocessing (kernel assigns one process to only one processor)
        LWP (Lightweight processes)
            Create extra layer between user and kernel threads
            LWP runs in user-space on top of kernel-level thread; multiple user-level threads can be created on top of each LWP
            LWPs of same process share data
        Process ~= Thread (Linux only)
            Schedulable entities are processes
            A process can be seen as a single thread, but a process can contain multiple threads that share code & data
            In pthreads library (NPTL), for linux, each thread created corresponds to kernel schedulable entity

Synchronization
    LOCKS
        - No assumptions on hardware
        - Mutual exclusion in maintained - only one thread can be executed inside a CS
        - Execution takes finite time
        - Thrd not in CS cannot prevent other threads from entering CS
        - Entering CS cannot be delayed indefinitely: no deadlock or starvation
        - Implementing locks at user-level  
            - Expensive to enter the kernel
            - Can't disable interrupts
            - Atomic read-modify-write instructions
                - Test and set
                    Atomically read variable and, if value of var is 0, set to 1
                - Fetch and increment
                    - Atomically return current val of mem location and incremeent val by 1
                - Compare and swap
                    - Atomically compare value of mem location w old value and if same replace w/ new value
                - Modern architectures perform atomic operations in the cache
        - Spin barriers

    CONDITION VARIABLES
        - Always associated w/ a condition and a lock
        - Used to wait for condition to take on a given value
        Wait(Lock lock)
            Release the lock
            Put thread object on wait queue of this CondVar Object
            Yield CPU to another thread
            When waken by system, reacquire lock and run
        Signal()
            If at least 1 thrd is sleeping on cond_var, wake 1 up. Otherwise, no effect
            Waking up a thrd means changing state to Ready and moving thrd object to run queue
        Broadcast()
            If 1 or more thrds are sleeping on cond_var, wake everyone up
            Otherwise, no effect

    SEMAPHORES
        Synchronized counting variables
        Formally, a semaphore comprises:
        An integer value
        Two operations:
        P()
        While value == 0, sleep
        Decrement value
        V()
        Increment value
        If there are any threads sleeping waiting for value to become non-zero, 
        wakeup at least 1 thread
        Using Semaphores
        Binary semaphores can be used to implement mutual exclusion
        Initialize counter to 1
        P == lock acquire
        V == lock release
        General semaphores (with the help of a binary semaphore) can 
        be used in producer-consumer types of synchronization problems
        Can you figure out how?  Hint: You should learn how to do it.
        Can you see how to implement semaphores given locks and 
        condition variables?
        Can you see how to implement locks and condition variables 
        given semaphores?
        Hint: if not, learn how
    MONITORS
        Semaphores have a few limitations: unstructured, difficult to 
        program correctly.  Monitors eliminate these limitations and are 
        as powerful as semaphores
        A monitor consists of a software module with one or more 
        procedures, an initialization sequence, and local data (can only 
        be accessed by procedures)
        Only one process can execute within the monitor at any one time 
        (mutual exclusion)  => entry queue
        Synchronization within the monitor implemented with condition 
        variables (wait/signal) => one queue per condition variable 
    DEADLOCKING
        - Occurs when multiple parties are competing for exclusive access to multiple resources
        - Prevention, Avoidance, Detection + Recovery to prevent deadlocks
        - Conditions for dead lock: mutual exclusion, hold and wait, no preemption, circular wait
            - Hold and wait - thrd/proc does not release a resource it has locked when it has to wait on another resource
            - No preemption - thrd/proc never has a resource that it is holding taken away
        PREVENTION
            - Design a system w/o one of mutual exclusion, hold/wait, no preemption or circular wait
        AVOIDANCE
            - Deny requests that may lead to unsafe states (Banker's)
            - Expensive to run on all resource requests
        DETECTION + RECOVERY
            - Check for circular wait periodically. If found, abort all deadlocked procs
            - Checking for circular wait is expensive
        BANKER'S ALGORITHM
            - Reject resource allocation requests that might leave system in "unsafe state"
            - State is safe if system can allocate resources to each process in some order and still avoid a deadlock
                Note that not all unsafe states are deadlock states
            - Algo is conservative and simply avoids unsafe states altogether
            - Process must declare the maximum resources it needs
            - When process requests resources, system checks whether allocation would leave system in an unsafe state
                - If so, process must wait until some other process releases enough resources

CPU Scheduling
    - Employs policies to determine scheduling (scheduling policies)
    - Preempting: Interrupting a job so that another can run
    - T_turnaround = T_completion - T_arrival (Metric to measure the time between job entering the queue and its completion)
    - T_response = T_firstrun - T_arrival (Metric to measure the time it takes between when it was submitted to when it is first run)
    FIFO - First In, First Out : First Come, First Served (FCFS)
        - Whatever job comes first gets run
        - PRO: Easy to implement
        - CON: Bad turnaround time, since a task that requires a long time to finish will hog the CPU,
                resulting in bad turnaround time (convoy effect: relatively short potential consumers gets
                queued behind a heavyweight)
    SJF - Shortest Job First
        - Runs the shortest job first
        - PRO: Good turn around time
        _ CON: Jobs do not all arrive together and a task that takes a long time to finish may arrive before
                shorter jobs and thus will lock out the shorter jobs. 
    STCF - Shortest Time-to-Completion First
        - The job that takes the shortest amount of time to finish is run first
        - Anytime a new job enters the system, the STCF scheduler determines which of the remaining jobs has the least time left,
            and schedules that one
        - STCF not really good for response time as the third job has to wait for the previous two jobs to run completely before
            being scheduled
        - PRO: Much better turnaround time
        - CON: Bad response time
    RR - Round Robin
        - Instead of running jobs to completion, RR runs a job for a time slice (or quantum), preempts it, and goes to the next task.
            Repeats until all jobs are finished
        - Length of time slice is critical for RR. Shorter time slices the better response time but too short and the cost of
            context-switching between jobs degrades performance. Length of time too long and we see the same problem as FIFO.
            Deciding on the time slice length presents a trade-off: making it long enough to amortize cost of switching without
            making it so long that the system is not responsive
        - RR is one of the worst policies if turnaround is our metric since it stretches one job as long as it can by running it
            in short time slices
        - Any policy that is fair (evenly divides the CPU amongst active processes on a small time scale) will perform poorly on metrics
            such as turnaround, utilization, response time, deadlines
        - SJF & STCF: Good turnaround, bad response time. RR: Good response time, bad turnaround

    - All programs perform I/O
    - OS must decide what to do about a process perfomring I/O as it is not actively using the CPU when it is doing I/O
        - Blocked until the I/O is done
    
    MLFQ - Multi-Level Feedback Queue
        - Wants to optimize turnaround time by running shortest jobs first but the OS does not generally know how long a job will run
            for. Wants to make a system feel responsive to ineractive users but that would result in bad turnaround time.
        - MLFQ has a number of distinct queues with each assigned a different priority level. A job is on one of these queues at one
            time only. Whenever a job needs to be run, the job with the highest priority (one in a higher queue) is chosen to run
        - Key to MLFQ is how scheduler sets priorities
        - Varies the priority on a job based on the job's observed behavior
            - If a job repeatedly relinquishes the CPU for keyboard input, MLFQ keeps its priority high so it will be more responsive
            - If a job uses the CPU intensively and does not constantly relinquish the CPU, it is set at a lower priority
            - Tries to learn about the history of the job to predict its future behavior
        - Mix of interactive and long-running "CPU-bound" jobs
        - When a job enters the system, it is placed at highest priority
        - If a job uses up an entire quantum, its priority is lowered
        - If a job gives up the CPU before time slice is up, it stays at the same priority level
        ISSUES
            - Too many interactive jobs will consume all CPU time, and thus long-running jobs will never receive any CPU time (starvation)
            - A smart user could rewrite their program to issue an IO operation and relinquish CPU and allow it to remain in the highest
                priority level and can nearly monopolize the CPU

        PRIORITY BOOST
            - Periodically boost priority of all jobs in the system
                - After some time period S, move all jobs to highest priority queue

        BETTER ACCOUNTING
            - Keeping track of the total amount of time slices that a job uses in a given queue, and when it uses up all of its time slices,
                it is moved to a lower priority queue
    MULTIPROCESSOR SCHEDULING

Memory Management
    Registers - extremely fast and extremely tiny (a single byte)
    Cache - Very fast, tiny compared to main memory (KB to MB), <50 cycles
    Memory - Slow, lots of space (GB), hundreds of cycles (200-400 cycles)
    VMEM

    Capacity Miss
    Conflict Miss
    Cold miss

    CACHE
        - Acts as attraction memory, storing values of RAM that were recently access (temporal locality)
        - Transfer between RAM and cache peerformed in cache blocks/lines
        - Mapping btwn caches and RAM is (mostly) staticc (fast handling of cache misses)
        - Often L1 I-cache is separate from D-cache

    PAGES
        - Cacheable unit of virtual memory
        - OS controls mapping btwn pages of VM and RAM
    - Hardware sees shared physical memory while software sees a private virtual addr space
    - Mem Mgmt in OS coordinates these two views
        - Consistency: all addr spaces should look "basically the same"
        - Relocation: processes can be loaded at any physical addr
        - Protection: a process cannot maliciously access memory belonging to another proc
        - Sharing: allow sharing of physical memory

    MEM. ALLOCATION
        - First-fit: allocate first big enough hole
        - Best-fit: allocate the smallest hole that is big enough. Produces smallest leftover hole.
        - Worst-fit: Allocate the largest hole. Produces biggest leftover hole.
        - First-fit and best-fit better than worst-fit in speed and storage utilization

    FRAGMENTATION
        - Entire processes are laoded into memory, lots of unused space, but too small for new jobs
        - Idea: Break processes into small, fixed-sized chunks (pages), so that processes don't have to exist contiguously
        - Segmentation: same idea but variable-sized chunks

    VIRTUAL MEMORY
        - Gives illusion that the addr is contiguous and may be larger than physical addr space
        - VM can be implemented using either paging or segmentation but paging is presently most common
        - VM is motivated by both
            - Convenience
            - Programmer does not ahve to deal with machines that have diff amount of physical memory
        - Translation from virtual to physical can be done in software but w/o protection
        - Hardware support is needed for protection & performance
            - Simplest solution with two registers: base and size
        
    SEGMENTATION
        - Segments are variable size
        - Transation done through set of (base, size, state) tuples - segment table indexed by seg. num.
        - State: valid/invalid, access permission, reference, modified bits
        - Segments may be visible to the programmer and can be used as a convenience for organizing
            the programs and data (code, segment, global data segment)
    
    PAGING
        - Pages are of fixed size
        - Physical memory correspond to a page is called a page frame
        - Translation done through a page table indexed by page number
        - Each entry in page table contains frame number that the vpage is mapped to and the state
            of the page in memory
        - State: valid/invalid, access permission, reference, modified, caching bits
        - Paging is transparent to the programmer
    
    COMBINED SEG and PAGING
        - Some MMUs combine paging w/ segmentation
        - VAddr: segment number + page number + offset
        - Segmentation translation is performed first
        - Segment entry points to a page table for that segment
        - Page no. portion of vaddr is used to index page table and look up cooresponding page frame no.
    
    TLB - TRANSLATION LOOKASIDE BUFFER
        - Translation on every mem. access must be fast
        - Cache for page table entries is called TLB
        - Traditionally, fully associative and single-level but are becoming set-associative and multi-level
        - Relatively small no. of entires
        - TLB entry contains page number and corresponding PT entry
        - On each mem. access, we look for page -> frame mapping in the TLB 
        - TLB Miss: TLB does not contain right PT entry
            - Evict existing entry if does not have any free ones
                - Pseudo-LRU common today: one bit represents each internal node of BST; cache lines are leaves;
                    access sets the bits to the other direction in the tree
            - Can be handled in hardware (CISC, x86) or in software (RISC, MIPS)
        - Vaddr space may be larger than physical memory
        - Stored on next advice down in memory hireacrhy

    PAGE TABLE STRUCTURE
        - Linear page tables can become huge
        - Sol. Two level PT: page the PT, Saves space by longly allocating 2nd level tabes for vmem that has actual
            been allocated. Does not need large contiguuous chunk of physical memory
        -Inverted PT: Saves lot of space, requires chunk of memory in proportion to size of physical addr space.
            Translation is done through hash table.
        - Vaddr is divided into:
            - Page number consisting of 20 bits, page offset of 12 bits
            - Since page table is paged, page number is further divided into 10bit page number, 10bit page offset
            +------------+-----------+-------------------------+
            |       Page number      |       Page offset       |
            +-----------+------------+-------------------------+
            |  p1       |    p2      |           d             |
            +-----------+------------+-------------------------+

            p1 is index of outer page table, p2 is displacement within page to which outer page table points
        - Translation from virtual to physical memory will take n memory addresses for n-level PT
        - TLB hit rate 98%, TLB access time of 2ns, memory access of 120ns, and a 2-level PT yield
            effective_access_time = 0.98(2+120) + 0.02(2+360) = 127ns
            6% slowdown in memory access time

        - IPT hases virtual page number + pid and the result indexes hash table. Each entry stores first entry to chain,
            virtual page number of each entry is compared to the referenced page number and on a match corresponding frame
            number is used
        - if VM <= physical memory, no problem
        - if VM > physical memory, part is stored in memory, part is stored on disk

        - Start process load code page where the process will be executing
        - As process references memory outside loaded page, bring in as necessary
        - How to represent fact that a page of VM is not in memory?
    PAGE FAULT
        - Page fault exception
            - Check the reference is valid and load the desired page from disk. Update page table entry to point to new frame,
                change valid bit of the page to v. Restart instruction that was interrupted
            - If no free frames
                - Free a frame currently being used
                - Select frame to be replaced
                - Change page table to reflect that victim is invalid
                - Read the desired page into the newly freed frame
                - Change the PT: new page is in the freed frame and now is valid
                - Restart faulting instruction
                - Optimization: Do not need to write victim back if it has not been modified (need dirty bit per page)
                - Motivated to find a good replacement policy
                    - Is there an optimal replacement algorithm?
                        - FIFO
                            - Each page lives in memory for about the same time
                        - LRU
                            - On page access, timestamp it
                            - When evicting, choose one with oldest timestamp
                            - Quite good for most programs
                        - NFUR - Not Frequently Used Replacement
                            - Have reference and counter for each page frame
                            - At each clock interrupt, OS adds the reference bit of each frame to its counter and clears ref bit
                            - When need to evict page, select frame with lowest counter value
                            - Will not evict a page used a lot in the past but is no longer accessed
                        - Clock (2nd chance)
                        - Nth-chance

        MULTIPROCESSOR SUPPORT
            - More than one addr space can be loaded to memory
            - CPU updaes register when context switching btwn threads from diff processes
            - Most TLBs can be cached more than one PT
                - Store pid to distinguish btwn virtual addrs belonging to diff procs
                - If TLB only caches one PT then it must be flushed at process-switch time
            - Sharing memory and copy-on-write
            - Page faults are expensive
                - OS better do a great job managing movement of data between 2nd storage and main memory
