Caching

Invoking the OS

I/O event notification

Send/receive data to I/O devices

Processes and Threads
    - Each process can have multiple threads
    - Each thread has a private stack
        - Registers are also private
    - All threads share code, globals, and heap
        - Objects shared across multiple threads should be in the heap or globals
    DIAGRAM 1

    SINGLE-THREADED ADDR SPACE

    +-----------------+
    |       OS        |
    +-----------------+
    |      Code       |
    +-----------------+
    |     Globals     |
    +-----------------+
    |                 |
    |      Stack      |
    |                 |
    +-----------------+
    |                 |
    +-----------------+
    |      Heap       |
    +-----------------+

    MULTI-THREADED ADDR SPACE

    +-----------------+
    |       OS        |
    +-----------------+
    |       Code      |
    +-----------------+
    |     Globals     |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Stack      |
    +-----------------+
    |      Heap       |
    +-----------------+

    - Each thread described by a thread-control block (TCB)
        - Contains
            - Thread ID
            - Space for saving registers
            - Thread-specific info (e.g. signal mask, scheduling priority)
    - Although model is each thread ahs private stack, threads actually share process addr space (NO MEMORY PROTECTION)
    - Threads could potentially write into each other's stacks

    RUNNING ON A SINGLE CORE NON-MULTITHREADED CPU wtf to do?
        - We multiplex multiple threads on the core
        - Only one thread runs at one time
        - OS may decide to stop currently running thread and allow another to run (called context-switching)
   
    CONTEXT SWITCHING
        - Copy all live registers to TCB
            - For register-only machines, need at least 1 scratch register
                - points to area of memory in TCB that registers should be saved to
        - Restore state of thread to run next
            - Copy values of live registers from TCB to registers
        - When does context switching take place?
            - When OS decides a thread has run long enough 
            - When thread is performing I/O
            - To wait for some other thread
        - At entry to context switch, return addr is either in a register or on the stack (in current activation record)
        - CS saves this return addr to the TCB instead of current PC
        - To thread, it looks like CS just took a while to return
        - Run queue that points to TCB of threads ready to run
        - Blocked queue per device to hold TCBs of threads blocked waiting for I/O on device to complete
        - When thread is switched out at timer interrupt, it is still ready to run so TCB stays on the run queue
        - When thread is switched out because it is blocking in I/O, TCB is moved to the blocked queue of the device

        SWITCHING CODE PATHWAY
            user thread executing -> clock interrupt -> PC modified by hardware to "vector" to interrrupt handler ->
            user thread state is saved for restart -> clock interrupt handler is invoked -> disable interrupt checking ->
            check whether current thread has run "long enough" -> if yes,m post asynchronous software trap (AST) ->
            enable interrupt checking -> exit clock interrupt handler -> enter "return to user" code -> check whether AST was posted ->
            if not, restore user thread state and return 4to executing user thread; if AST was posted, call context switch code

        ALTERNATIVE
            user thread executing -> system call to perform I/O -> PC modified by hardware to "vector" to trap handler ->
            user thread state saved for restart -> OS code to perform system call is invoked -> disable interrupt checking ->
            I/O operation started (by invoking I/O driver) -> set thread status to waiting -> move thread's TCB from run queue to wait
            queue associates with specific device -> enable interrupt checking -> exit trap handler -> call context switching code

        - At entry to CS, return addr is either in register or on the stack (in the current activation record). CS saves return addr to TCB
        instead of current PC
        - A run queue points to TCBs of threads ready to run
        - Blocked queue per device to holdw TCBs of threads blocked waiting for an I/O operation on that device
        - When thread is switched out at timer interrupt it is still ready to run so its TCB stays on the run queue
        - When a thread is switched out b/c it is blocking on I/O, TCB is moved to blocked queue of that device
        
        SWITCHING TO THREAD OF DIFF. PROCESS
        - Caches:
            Physical addresses: no issue
            Virtual addrs: cache must either have process tag or must flush cache on CS
        - TLB
            Each entry must have process tag or must flush TLB on CS
        - Page Table
            Typically have page table pointer (register) that must be reloaded on CS
        
        What happens if kernel wants to signal a process when all of its threads are locked?
        When there are multiple threads, which thread should the kernel deliver the signal to?
            - OS writes into PCB that signal should be deliviered
            - Next time thread from process is allowed to run, signal is delivered to that thread as part of the CS
        What happens if kernel needs to deliver multiple signals?
        
        Kernel-level threads
            Sees multiple execution contexts
            Thread management done by kernel
        User-level threads
            Implemented a sa thread library
            Kernel sees one execution context and is unaware of thread activity
            Can be preemptive or not

        Advantages of ULT
            Performance: low-cost thread operations
            Flexibility: scheduling can be application specific
            Portability: user-level thread library easy to port
        Disadvantages:
            If a user-level thread blocks in kernel entire process is blocked
            Cannot take advantage of multiprocessing (kernel assigns one process to only one processor)
        LWP (Lightweight processes)
            Create extra layer between user and kernel threads
            LWP runs in user-space on top of kernel-level thread; multiple user-level threads can be created on top of each LWP
            LWPs of same process share data
        Process ~= Thread (Linux only)
            Schedulable entities are processes
            A process can be seen as a single thread, but a process can contain multiple threads that share code & data
            In pthreads library (NPTL), for linux, each thread created corresponds to kernel schedulable entity
Synchronization

CPU Scheduling
    - Employs policies to determine scheduling (scheduling policies)
    - Preempting: Interrupting a job so that another can run
    - T_turnaround = T_completion - T_arrival (Metric to measure the time between job entering the queue and its completion)
    - T_response = T_firstrun - T_arrival (Metric to measure the time it takes between when it was submitted to when it is first run)
    FIFO - First In, First Out : First Come, First Served (FCFS)
        - Whatever job comes first gets run
        - PRO: Easy to implement
        - CON: Bad turnaround time, since a task that requires a long time to finish will hog the CPU,
                resulting in bad turnaround time (convoy effect: relatively short potential consumers gets
                queued behind a heavyweight)
    SJF - Shortest Job First
        - Runs the shortest job first
        - PRO: Good turn around time
        _ CON: Jobs do not all arrive together and a task that takes a long time to finish may arrive before
                shorter jobs and thus will lock out the shorter jobs. 
    STCF - Shortest Time-to-Completion First
        - The job that takes the shortest amount of time to finish is run first
        - Anytime a new job enters the system, the STCF scheduler determines which of the remaining jobs has the least time left,
            and schedules that one
        - STCF not really good for response time as the third job has to wait for the previous two jobs to run completely before
            being scheduled
        - PRO: Much better turnaround time
        - CON: Bad response time
    RR - Round Robin
        - Instead of running jobs to completion, RR runs a job for a time slice (or quantum), preempts it, and goes to the next task.
            Repeats until all jobs are finished
        - Length of time slice is critical for RR. Shorter time slices the better response time but too short and the cost of
            context-switching between jobs degrades performance. Length of time too long and we see the same problem as FIFO.
            Deciding on the time slice length presents a trade-off: making it long enough to amortize cost of switching without
            making it so long that the system is not responsive
        - RR is one of the worst policies if turnaround is our metric since it stretches one job as long as it can by running it
            in short time slices
        - Any policy that is fair (evenly divides the CPU amongst active processes on a small time scale) will perform poorly on metrics
            such as turnaround
        - SJF & STCF: Good turnaround, bad response time. RR: Good response time, bad turnaround

    - All programs perform I/O
    - OS must decide what to do about a process perfomring I/O as it is not actively using the CPU when it is doing I/O
        - Blocked until the I/O is done
    
    MLFQ - Multi-Level Feedback Queue
        - Wants to optimize turnaround time by running shortest jobs first but the OS does not generally know how long a job will run
            for. Wants to make a system feel responsive to ineractive users but that would result in bad turnaround time.
        - MLFQ has a number of distinct queues with each assigned a different priority level. A job is on one of these queues at one
            time only. Whenever a job needs to be run, the job with the highest priority (one in a higher queue) is chosen to run
        - Key to MLFQ is how scheduler sets priorities
        - Varies the priority on a job based on the job's observed behavior
            - If a job repeatedly relinquishes the CPU for keyboard input, MLFQ keeps its priority high so it will be more responsive
            - If a job uses the CPU intensively and does not constantly relinquish the CPU, it is set at a lower priority
            - Tries to learn about the history of the job to predict its future behavior
        - Mix of interactive and long-running "CPU-bound" jobs
        - When a job enters the system, it is placed at highest priority
        - If a job uses up an entire quantum, its priority is lowered
        - If a job gives up the CPU before time slice is up, it stays at the same priority level
        ISSUES
            - Too many interactive jobs will consume all CPU time, and thus long-running jobs will never receive any CPU time (starvation)
            - A smart user could rewrite their program to issue an IO operation and relinquish CPU and allow it to remain in the highest
                priority level and can nearly monopolize the CPU

        PRIORITY BOOST
            - Periodically boost priority of all jobs in the system
                - After some time period S, move all jobs to highest priority queue

        BETTER ACCOUNTING
            - Keeping track of the total amount of time slices that a job uses in a given queue, and when it uses up all of its time slices,
                it is moved to a lower priority queue
    MULTIPROCESSOR SCHEDULING

Memory Management
    Registers - extremely fast and extremely tiny (a single byte)
    Cache - Very fast, tiny compared to main memory (KB to MB), <50 cycles
    Memory - Slow, lots of space (GB), hundreds of cycles (200-400 cycles)
    VMEM

    Capacity Miss
    Conflict Miss
    Cold miss

    CACHE
        - Acts as attraction memory, storing values of RAM that were recently access (temporal locality)
        - Transfer between RAM and cache peerformed in cache blocks/lines
        - Mapping btwn caches and RAM is (mostly) staticc (fast handling of cache misses)
        - Often L1 I-cache is separate from D-cache

    PAGES
        - Cacheable unit of virtual memory
        - OS controls mapping btwn pages of VM and RAM
    - Hardware sees shared physical memory while software sees a private virtual addr space
    - Mem Mgmt in OS coordinates these two views
        - Consistency: all addr spaces should look "basically the same"
        - Relocation: processes can be loaded at any physical addr
        - Protection: a process cannot maliciously access memory belonging to another proc
        - Sharing: allow sharing of physical memory

    MEM. ALLOCATION
        - First-fit: allocate first big enough hole
        - Best-fit: allocate the smallest hole that is big enough. Produces smallest leftover hole.
        - Worst-fit: Allocate the largest hole. Produces biggest leftover hole.
        - First-fit and best-fit better than worst-fit in speed and storage utilization

    FRAGMENTATION
        - Entire processes are laoded into memory, lots of unused space, but too small for new jobs
        - Idea: Break processes into small, fixed-sized chunks (pages), so that processes don't have to exist contiguously
        - Segmentation: same idea but variable-sized chunks

    VIRTUAL MEMORY
        - Gives illusion that the addr is contiguous and may be larger than physical addr space
        - VM can be implemented using either paging or segmentation but paging is presently most common
        - VM is motivated by both
            - Convenience
            - Programmer does not ahve to deal with machines that have diff amount of physical memory
        - Translation from virtual to physical can be done in software but w/o protection
        - Hardware support is needed for protection & performance
            - Simplest solution with two registers: base and size
        
    SEGMENTATION
        - Segments are variable size
        - Transation done through set of (base, size, state) tuples - segment table indexed by seg. num.
        - State: valid/invalid, access permission, reference, modified bits
        - Segments may be visible to the programmer and can be used as a convenience for organizing
            the programs and data (code, segment, global data segment)
    
    PAGING
        - Pages are of fixed size
        - Physical memory correspond to a page is called a page frame
        - Translation done through a page table indexed by page number
        - Each entry in page table contains frame number that the vpage is mapped to and the state
            of the page in memory
        - State: valid/invalid, access permission, reference, modified, caching bits
        - Paging is transparent to the programmer
    
    COMBINED SEG and PAGING
        - Some MMUs combine paging w/ segmentation
        - VAddr: segment number + page number + offset
        - Segmentation translation is performed first
        - Segment entry points to a page table for that segment
        - Page no. portion of vaddr is used to index page table and look up cooresponding page frame no.
    
    TLB - TRANSLATION LOOKASIDE BUFFER
        - Translation on every mem. access must be fast
        - Cache for page table entries is called TLB
        - Traditionally, fully associative and single-level but are becoming set-associative and multi-level
        - Relatively small no. of entires
        - TLB entry contains page number and corresponding PT entry
        - On each mem. access, we look for page -> frame mapping in the TLB 
        - TLB Miss: TLB does not contain right PT entry
            - Evict existing entry if does not have any free ones
                - Pseudo-LRU common today: one bit represents each internal node of BST; cache lines are leaves;
                    access sets the bits to the other direction in the tree
            - Can be handled in hardware (CISC, x86) or in software (RISC, MIPS)
        - Vaddr space may be larger than physical memory
        - Stored on next advice down in memory hireacrhy

    PAGE TABLE STRUCTURE
        - Linear page tables can become huge
        - Sol. Two level PT: page the PT, Saves space by longly allocating 2nd level tabes for vmem that has actual
            been allocated. Does not need large contiguuous chunk of physical memory
        -Inverted PT: Saves lot of space, requires chunk of memory in proportion to size of physical addr space.
            Translation is done through hash table.
            - Implementation decreases memory needed to store PT but also slower access time, therefore searching is not
                used
            - Can use hash table to reduce look up. Under hashing, each PT also includes a frame number and a chain pointer
            
